X=[[0.78051,-0.063669],[0.28774,0.29139]]
y=[1,1]
prediction(X,y,2)

import numpy as np

# Write a function that takes as input a list of numbers, and returns
# the list of values given by the softmax function.
def softmax(L):
    sum=0
    out=[]
    for i in range(len(L)):
        sum = sum + np.exp(L[i])
    for i in range(len(L)):
        out.append(np.exp(L[i])/sum)
    return out

L = [2,1,0]
softmax(L)

#Cross Entropy
import numpy as np

# Write a function that takes as input two lists Y, P,
# and returns the float corresponding to their cross-entropy.
def cross_entropy(Y, P):
    Y=np.float_(Y)
    P=np.float_(P)
    out = sum(Y*np.log(P) + (1-Y)*np.log(1-P))
    
    return out

Y=[1,0]
P=[0.8,0.4]
cross_entropy(Y,P)
